[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nAishwarya\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nAishwarya\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nProbability Theory\n\n\nRandom Variables\n\n\nMachine Learning Applications\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nAishwarya\n\n\n\n\n\n\n  \n\n\n\n\nUnraveling the Mysteries of Data: A Journey into Clustering with Python\n\n\n\n\n\n\n\nclustering\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nAishwarya\n\n\n\n\n\n\n  \n\n\n\n\nClassifying Iris Flowers with Machine Learning\n\n\n\n\n\n\n\nclassification\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nAishwarya\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Probability Theory and Machine Learning: A Practical Guide with Python\nIn the realm of data science, probability theory plays a fundamental role in understanding the underlying patterns and relationships within data. It provides a powerful framework for quantifying uncertainty and making informed decisions based on statistical analysis. Machine learning algorithms, which are at the heart of modern artificial intelligence, heavily rely on probability concepts to effectively model and predict outcomes.\nIn this blog post, we will embark on a practical journey through probability theory and machine learning using Python, a versatile programming language widely used in data science applications. We will delve into the concepts of random variables, probability distributions, and Gaussian Naive Bayes classification, while also exploring linear regression and its applications in predictive modeling.\nExploring Random Variables and Probability Distributions\nRandom variables are the cornerstone of probability theory, representing quantities that can take on different values with associated probabilities. They are often represented by letters like X or Y, and their possible values are collectively known as the sample space.\nProbability distributions, on the other hand, describe the likelihood of different values for a random variable. They are the foundation for making inferences about the behavior of random phenomena. Common probability distributions include the normal distribution, the uniform distribution, and the binomial distribution.\nGaussian Naive Bayes Classification\nGaussian Naive Bayes is a classification algorithm that assumes a Gaussian (normal) distribution for each feature in each class. It is a simple yet effective algorithm that is well-suited for problems with a small number of features and a large number of training examples.\nTo illustrate the application of Gaussian Naive Bayes, consider the Iris dataset, which contains measurements of the sepal and petal length and width of three different species of irises. We can use Gaussian Naive Bayes to classify new irises based on their measurements.\nImplementing Gaussian Naive Bayes in Python\nThe provided Python code demonstrates the implementation of Gaussian Naive Bayes for Iris classification. It includes steps for loading the Iris dataset, splitting the data into training and testing sets, training the Gaussian Naive Bayes model, evaluating the model’s performance, and visualizing the results using confusion matrix and classification report.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Load the Iris dataset from a CSV file\niris_df = pd.read_csv('Iris.csv')  # Replace 'your_iris_dataset.csv' with the actual file path\n\n# Split the data into features (X) and target variable (y)\nX = iris_df.drop('Species', axis=1)\ny = iris_df['Species']\n\n\n\n\nCode\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Gaussian Naive Bayes Classifier\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\n\nCode\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Visualize the Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g', cbar=False)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Visualize the Classification Report\nreport_df = pd.DataFrame.from_dict(classification_report(y_test, y_pred, output_dict=True))\nplt.figure(figsize=(8, 6))\nsns.heatmap(report_df.iloc[:-1, :].T, annot=True, cmap='Blues', fmt='.2f', cbar=False)\nplt.title('Classification Report')\nplt.show()\n\n\n\n\n\nThis code demonstrates the simplicity and effectiveness of Gaussian Naive Bayes classification for a classification task with a small number of features. The confusion matrix and classification report provide insights into the model’s performance, indicating its ability to correctly classify the different Iris species. Exploring Random Variables and Target Variables: A Linear Regression Approach\nIn the realm of machine learning and statistical analysis, random variables play a pivotal role in understanding the relationships between different data points. They represent quantities that can take on various values with associated probabilities, forming the basis for statistical modeling and predictive analysis.\nIn this section, we will explore the concept of random variables and their influence on a target variable, demonstrating the application of linear regression to model the relationship between these variables.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\nCreating Synthetic Data with Random Variables\nTo illustrate the concepts, we will create a synthetic dataset consisting of two random variables, Random Variable 1 and Random Variable 2, along with a target variable, Target Variable. The target variable will be generated from a linear combination of the two random variables, incorporating additional noise to simulate real-world scenarios.\n\n\nCode\n# Create synthetic data with random variables\nnp.random.seed(42)\nsize = 1000\nrandom_variable_1 = np.random.normal(0, 1, size)\nrandom_variable_2 = np.random.uniform(0, 1, size)\ntarget_variable = 2 * random_variable_1 - 3 * random_variable_2 + np.random.normal(0, 1, size)\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'Random Variable 1': random_variable_1,\n    'Random Variable 2': random_variable_2,\n    'Target Variable': target_variable\n})\n\n\nVisualizing Relationships between Random Variables and Target Variable\nTo gain insights into the relationships between the random variables and the target variable, we will utilize seaborn’s pairplot function. This tool allows us to visualize the distribution of each variable and identify any potential correlations between them.\n\n\nCode\n# Visualize the relationship between the random variables and the target variable\nsns.pairplot(df)\nplt.show()\n\n# Split the data into features (X) and target variable (y)\nX = df[['Random Variable 1', 'Random Variable 2']]\ny = df['Target Variable']\n\n\n\n\n\nSplitting Data into Training and Testing Sets\nBefore applying linear regression, we will split the synthetic data into training and testing sets. The training set will be used to fit the linear regression model, while the testing set will be used to evaluate the model’s performance on unseen data.\n\n\nCode\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nTraining a Linear Regression Model\nLinear regression is a statistical method that assumes a linear relationship between the independent variables (random variables) and the dependent variable (target variable). By fitting a linear model to the training data, we can estimate the coefficients that represent the contribution of each random variable to the target variable.\n\n\nCode\n# Train a Linear Regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nMaking Predictions and Evaluating Model Performance\nUsing the trained linear regression model, we can make predictions for the target variable based on the values of the random variables in the testing set. To assess the model’s performance, we will calculate the mean squared error (MSE), which measures the average squared difference between the predicted and actual values of the target variable.\n\n\nCode\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse}')\n\n\nMean Squared Error: 0.8492309009763218\n\n\nVisualizing Predicted vs. Actual Values\nTo further evaluate the model’s performance, we will create a scatter plot comparing the predicted values of the target variable to the actual values. This visualization provides a visual representation of how well the model captures the underlying relationship between the random variables and the target variable.\n\n\nCode\n# Visualize the predicted vs. actual values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Linear Regression: Predicted vs. Actual')\nplt.show()"
  },
  {
    "objectID": "posts/linear/index.html",
    "href": "posts/linear/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Linear Regression:\nLinear Regression is a fundamental machine learning algorithm used for predicting a continuous target variable based on one or more predictor variables. In this example, we’ll explore a Linear Regression model using a dataset on advertising spending and sales.\n\n\nCode\n# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import the numpy and pandas package\n\nimport numpy as np\nimport pandas as pd\n\n# Data Visualisation\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nadvertising = pd.DataFrame(pd.read_csv(\"advertising.csv\"))\nadvertising.head()\nadvertising.shape\nadvertising.info()\nadvertising.describe()\n# Checking Null values\nadvertising.isnull().sum()*100/advertising.shape[0]\n# There are no NULL values in the dataset, hence it i\n# Outlier Analysis\nfig, axs = plt.subplots(3, figsize = (5,5))\nplt1 = sns.boxplot(advertising['TV'], ax = axs[0])\nplt2 = sns.boxplot(advertising['Newspaper'], ax = axs[1])\nplt3 = sns.boxplot(advertising['Radio'], ax = axs[2])\nplt.tight_layout()\nsns.boxplot(advertising['Sales'])\nplt.show()\n# Let's see how Sales are related with other variables using scatter plot.\nsns.pairplot(advertising, x_vars=['TV', 'Newspaper', 'Radio'], y_vars='Sales', height=4, aspect=1, kind='scatter')\nplt.show()\n# Let's see the correlation between different variables.\nsns.heatmap(advertising.corr(), cmap=\"YlGnBu\", annot = True)\nplt.show()\nX = advertising['TV']\ny = advertising['Sales']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)\n# Let's now take a look at the train dataset\n\nX_train.head()\ny_train.head()\nimport statsmodels.api as sm\n# Add a constant to get an intercept\nX_train_sm = sm.add_constant(X_train)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y_train, X_train_sm).fit()\nplt.scatter(X_train, y_train)\nplt.plot(X_train, 6.948 + 0.054*X_train, 'r')\nplt.show()\n\n# Add a constant to X_test\nX_test_sm = sm.add_constant(X_test)\n\n# Predict the y values corresponding to X_test_sm\ny_pred = lr.predict(X_test_sm)\ny_pred.head()\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n#Returns the mean squared error; we'll take a square root\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n\n\nplt.scatter(X_test, y_test)\nplt.plot(X_test, 6.948 + 0.054 * X_test, 'r')\nplt.show()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   TV         200 non-null    float64\n 1   Radio      200 non-null    float64\n 2   Newspaper  200 non-null    float64\n 3   Sales      200 non-null    float64\ndtypes: float64(4)\nmemory usage: 6.4 KB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Regression:\nRandom Forest Regression is an ensemble learning method that combines the predictions of multiple decision tree models. Here, we’ll apply Random Forest Regression to the California Housing dataset.\n\n\nCode\n# 1. Importing libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# 2. Loading dataset\ncalifornia_housing = fetch_california_housing()\nX, y = california_housing.data, california_housing.target\n\n# 3. Splitting for training and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 4. Fitting the Random Forest Regression\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 5. Predicting values\ny_pred_train = rf_model.predict(X_train)\ny_pred_test = rf_model.predict(X_test)\n\n# 6. Visualization\nplt.scatter(y_test, y_pred_test)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Random Forest Regression - California Housing')\nplt.show()\n\n\n\n\n\nIn both cases, we explore the datasets, perform necessary pre-processing, train the models, and evaluate their performance using metrics like Root Mean Squared Error. The scatter plots visualize the predicted vs. actual values, providing insights into the models’ effectiveness in capturing relationships within the data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! I’m passionate about data science and machine learning, and I love sharing my knowledge with others. In this blog, I write about a variety of topics, including:\nProbability theory and random variables Clustering Linear and nonlinear regression Classification Anomaly Detection I believe that data science has the power to solve some of the world’s most pressing problems. By sharing my knowledge, I hope to empower others to use data science to make a positive impact."
  },
  {
    "objectID": "posts/Anomaly/index.html",
    "href": "posts/Anomaly/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly detection is a crucial aspect of data analysis, particularly in domains where identifying rare and potentially fraudulent events is paramount. In this analysis, we’ll explore the application of the Isolation Forest algorithm for anomaly detection in credit card transactions. The dataset used here is the Credit Card Fraud Detection dataset, which contains features derived from credit card transactions, including both legitimate and fraudulent ones.\nLoading the Dataset:\nLet’s begin by importing the necessary libraries and loading the Credit Card Fraud Detection dataset. This dataset is a curated collection of credit card transactions, where the ‘Class’ column denotes whether a transaction is normal (0) or fraudulent (1).\n\n\nCode\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Load the Credit Card Fraud Detection dataset\ndf = pd.read_csv('creditcard.csv')\n\n# Display the first few rows of the dataframe\nprint(df.head())\n\n# Explore the dataset\n#print(df.info())\n\n\n   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]\n\n\nThe ‘info()’ method provides an overview of the dataset, including the data types and the presence of any missing values.\nData Exploration:\nBefore delving into anomaly detection, it’s crucial to understand the composition of the dataset. Checking for missing values, exploring basic statistics, and visualizing the distribution of features can offer valuable insights into the characteristics of normal and fraudulent transactions.\nFeature Extraction:\nNext, we extract the features (X) and the target variable (y) from the dataset. The target variable ‘Class’ indicates whether a transaction is normal (0) or fraudulent (1).\n\n\nCode\n# Extract the features and target variable\nX = df.drop(['Class'], axis=1)\ny = df['Class']\n\n\nTrain-Test Split:\nTo assess the performance of the Isolation Forest algorithm, we split the dataset into training and testing sets. The training set will be used to fit the model, and the testing set will be employed to evaluate its ability to identify anomalies.\n\n\nCode\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nIsolation Forest Model:\nThe Isolation Forest algorithm is well-suited for anomaly detection tasks. It works by isolating anomalies rather than profiling normal instances. In this case, the contamination parameter is set to 0.01, indicating that approximately 1% of the data is expected to be fraudulent.\n\n\nCode\n# Isolation Forest model\nmodel = IsolationForest(contamination=0.01, random_state=42)\n\n# Fitting the model\nmodel.fit(X_train)\n\n\nIsolationForest(contamination=0.01, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(contamination=0.01, random_state=42)\n\n\nPrediction and Evaluation:\nThe model is then used to predict outliers on the test set. The predictions are converted to binary labels (1 for normal transactions and -1 for anomalies), and the model’s performance is evaluated using metrics such as confusion matrix and classification report.\n\n\nCode\n# Predicting outliers on the test set\ny_pred_test = model.predict(X_test)\n\n# Convert predictions to binary labels (1 for normal, -1 for outliers)\ny_pred_test_binary = np.where(y_pred_test == 1, 0, 1)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred_test_binary))\nprint(classification_report(y_test, y_pred_test_binary))\n\n\n[[56318   546]\n [   36    62]]\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99     56864\n           1       0.10      0.63      0.18        98\n\n    accuracy                           0.99     56962\n   macro avg       0.55      0.81      0.59     56962\nweighted avg       1.00      0.99      0.99     56962\n\n\n\nVisualization:\nLastly, a scatter plot is created to visualize the results of the Isolation Forest algorithm. Each data point is represented based on its index and the ‘V1’ feature, with normal transactions in one color and anomalies in another.\n\n\nCode\n# Visualize the results\nplt.scatter(range(len(X_test)), X_test['V1'], s=5, c=y_pred_test_binary, cmap='viridis', label='Data Points')\nplt.title('Isolation Forest Outlier Detection on Credit Card Fraud Data')\nplt.xlabel('Index')\nplt.ylabel('Feature: V1')\nplt.legend()\nplt.show()\n\n\n\n\n\nThis scatter plot provides an intuitive view of how well the Isolation Forest algorithm distinguishes between normal and anomalous credit card transactions based on the ‘V1’ feature. Overall, this analysis demonstrates the application of anomaly detection in a real-world scenario, showcasing the importance of such techniques in identifying potential fraud."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Classifying Iris Flowers with Machine Learning",
    "section": "",
    "text": "Classifying Iris Flowers with Machine Learning Machine learning classification involves training models to predict categorical labels or “classes”. In this post, we’ll walk through an example of classifying Iris flowers into three species (Setosa, Versicolor, Virginica) using Python.\nThe Iris dataset contains numeric measurements of sepal and petal dimensions for 150 flowers across 3 Iris species. It is a classic dataset for testing classification techniques. Our task will be to train a model to predict the Iris species based on the measurements.\nWe’ll use the K-Nearest Neighbors (KNN) algorithm, one of the simplest machine learning models. KNN makes predictions by comparing new data points to the training data and identifying the closest matches or “nearest neighbors”. Here we import libraries for data manipulation, model training, and metrics.\n\n\nCode\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#to split the dataset for training and testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier \n # for K nearest neighbours\nfrom sklearn import metrics #for checking the model accuracy\n\n\nNumPy provides numerical operations, pandas enables data analysis and manipulation, seaborn/matplotlib are for visualization, scikit-learn contains our modeling and metrics tools. We load the Iris CSV data into a pandas DataFrame and confirm the columns and data types.\n\n\nCode\niris = pd.read_csv(\"C:/Users/aishw/Desktop/Kuma/posts/post-with-code/Iris.csv\")\niris.head(2)\niris.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Id             150 non-null    int64  \n 1   SepalLengthCm  150 non-null    float64\n 2   SepalWidthCm   150 non-null    float64\n 3   PetalLengthCm  150 non-null    float64\n 4   PetalWidthCm   150 non-null    float64\n 5   Species        150 non-null    object \ndtypes: float64(4), int64(1), object(1)\nmemory usage: 7.2+ KB\n\n\nPrinting the head and information gives us a quick overview of the data - we can see there are 150 rows and 5 columns corresponding to 4 measurements for each flower plus the species name. The Id column is just an index so we drop it from the DataFrame.\n\n\nCode\niris.drop('Id',axis=1,inplace=True)\n\n\nNext we do some exploratory data analysis… Now we can take a look at some basic exploratory visualizations to understand characteristics and relationships in the Iris data: Plotting this way allows us to visually separate the different Iris species before even training a model. We observe differences and similarities across the measurements.\n\n\nCode\n# Scatter plots colored by species \n# Histograms for each feature\n# Violin plots showing feature distributions\nfig = iris[iris.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()\n\n\n\n\n\n\n\nCode\niris.hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.violinplot(x='Species',y='PetalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.violinplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.violinplot(x='Species',y='SepalWidthCm',data=iris)\n\n\n&lt;AxesSubplot:xlabel='Species', ylabel='SepalWidthCm'&gt;\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(7,4)) \nsns.heatmap(iris.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()\n\n\n\n\n\nThese visualizations allow us to understand characteristics and relationships in the data.\nWith some intuition about the data, we are ready to prepare it for modeling. First we split the data into training and test sets:\n\n\nCode\ntrain, test = train_test_split(iris, test_size = 0.3)# in this our main data is split into train and test\n# the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\nprint(train.shape)\nprint(test.shape)\n\n\n(105, 5)\n(45, 5)\n\n\nThe test size of 0.3 means 70% of rows will be used for training and 30% for final evaluation. Now we extract just the feature columns (measurements) into the X (independent variable) and the species column as y (dependent variable). Now we extract the feature and target columns:\n\n\nCode\ntrain_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]# taking the training data features\n\n\ntrain_y=train.Species# output of our training data\ntest_X= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] # taking test data features\ntest_y =test.Species   #output value of test data\ntrain_y.head()\n\n\n109     Iris-virginica\n58     Iris-versicolor\n64     Iris-versicolor\n9          Iris-setosa\n144     Iris-virginica\nName: Species, dtype: object\n\n\nTime to build our machine learning model! We will train a K-Nearest Neighbors classifier, one of the simplest models. KNN makes predictions based on finding similar labeled examples in historical data:\n\n\nCode\nmodel=KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_y))\nprint('done')\n\n\nThe accuracy of the KNN is 0.9555555555555556\ndone"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Unraveling the Mysteries of Data: A Journey into Clustering with Python",
    "section": "",
    "text": "In the realm of machine learning, clustering stands as an unsupervised learning technique, tasked with uncovering hidden patterns and groupings within data. It’s like an explorer venturing into uncharted territory, seeking to identify communities amidst a sea of unlabeled observations. Imagine a vast collection of customer data, each record representing an individual’s purchasing behavior. Clustering algorithms, like adept detectives, can sift through this data, discerning distinct groups of customers with similar purchasing patterns. This newfound knowledge can empower businesses to tailor marketing campaigns and product offerings to specific customer segments, enhancing customer satisfaction and profitability.\nA Glimpse into the World of Wine\nFor this exploration, we’ll delve into the world of wine, using a dataset that contains measurements of various chemical and sensory properties of different wine types. Our goal is to uncover hidden patterns in this data and understand how these patterns relate to the quality of the wine.\n\n\nCode\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\n\n\nStep 1: Data Loading - Gathering the Essentials\nOur journey begins with loading the wine dataset into our Python environment. This involves utilizing the pandas library, a powerful tool for data manipulation and analysis. With the data securely in hand, we can proceed to extract the relevant features, the chemical and sensory properties that will serve as the basis for our clustering analysis.\n\n\nCode\ndata = pd.read_csv('C:/Users/aishw/Desktop/Kuma/posts/welcome/winequality-red.csv')\ndata.head(3)\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1599 entries, 0 to 1598\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1599 non-null   float64\n 1   volatile acidity      1599 non-null   float64\n 2   citric acid           1599 non-null   float64\n 3   residual sugar        1599 non-null   float64\n 4   chlorides             1599 non-null   float64\n 5   free sulfur dioxide   1599 non-null   float64\n 6   total sulfur dioxide  1599 non-null   float64\n 7   density               1599 non-null   float64\n 8   pH                    1599 non-null   float64\n 9   sulphates             1599 non-null   float64\n 10  alcohol               1599 non-null   float64\n 11  quality               1599 non-null   int64  \ndtypes: float64(11), int64(1)\nmemory usage: 150.0 KB\n\n\nStep 2: Feature Extraction - Selecting the Relevant Pieces\nThe wine dataset contains a wealth of information, but not all of it is relevant to our clustering task. We carefully select the features that are most likely to contribute to meaningful groupings, such as alcohol content, acidity levels, and residual sugar content. These features will represent each wine sample in our analysis.\n\n\nCode\n# Step 2: Feature Extraction - Selecting the Relevant Pieces\n\nfeatures = data[['fixed acidity',   'volatile acidity', 'citric acid',  'residual sugar',   'chlorides',    'free sulfur dioxide', 'total sulfur dioxide',  'density',  'pH',   'sulphates',    'alcohol',  'quality']]\n\n\nStep 3: Data Preprocessing - Preparing for the Algorithm\nBefore we can unleash the clustering algorithm, we need to ensure that our data is in a suitable format. This involves a process called standardization, where we scale the features to a similar range. This step is crucial as it prevents any single feature from dominating the clustering process.\n\n\nCode\n# Step 3: Data Preprocessing - Preparing for the Algorithm\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(features)\n\n\nStep 4: KMeans Model Creation - Unveiling the Clusters\nNow we introduce the K-means algorithm, our chosen tool for uncovering hidden patterns in the wine data. K-means is a partitioning-based clustering algorithm that works by iteratively assigning data points to clusters based on their proximity to cluster centroids. The number of clusters, known as ‘k’, is a crucial parameter that determines the granularity of the clustering. In our case, we set k to three, corresponding to the expected number of wine types in the dataset.\n\n\nCode\n# Step 4: KMeans Model Creation - Unveiling the Clusters\n# Create the KMeans model and specify the number of clusters (3, matching the number of wine types)\nkmeans = KMeans(n_clusters=3)\n\n\nStep 5: Model Fitting - Assigning Cluster Labels\nWith the K-means model ready, we fit it to our preprocessed data. This involves iteratively updating the cluster centroids and assigning data points to the nearest centroid until convergence is reached. As a result, each data point is assigned a cluster label, indicating its membership in one of the three identified groups.\n\n\nCode\n# Step 5: Model Fitting - Assigning Cluster Labels\n\n# Fit the model to the data\nkmeans.fit(standardized_data)\n\n\nC:\\Users\\aishw\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKMeans(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3)\n\n\nStep 6: Cluster Prediction - Identifying Cluster Membership\nOnce the model is fitted, we can predict the cluster labels for any new wine sample. This allows us to classify new wines into the existing groupings, providing insights into their characteristics and potential quality.\n\n\nCode\n# Step 6: Cluster Prediction - Identifying Cluster Membership\n\n# Predict the cluster labels for each data point\ncluster_labels = kmeans.predict(standardized_data)\n\n\nStep 7: Cluster Evaluation - Assessing the Algorithm’s Performance\nTo evaluate the effectiveness of our clustering, we employ the silhouette score, a metric that measures the separation and compactness of clusters. A higher silhouette score indicates better clustering, with values ranging from -1 to 1.\n\n\nCode\n# Step 7: Cluster Evaluation - Assessing the Algorithm's Performance\n\n# Evaluate the clustering performance using silhouette score\nfrom sklearn.metrics import silhouette_score\nsilhouette_avg = silhouette_score(features, cluster_labels, sample_size=len(features))\nprint(\"Silhouette score:\", silhouette_avg)\n\n\nSilhouette score: 0.06178140210994517\n\n\nStep 8: Visualization - Bringing Clusters to Life\nTo gain a visual understanding of the clusters, we utilize Principal Component Analysis (PCA) to reduce the dimensionality of the data from 11 features to 2 dimensions. This allows us to plot the data points in a 2D scatter plot, where different colors represent different clusters.\n\n\nCode\n# Step 8: Visualization - Bringing Clusters to Life\n\n# Visualize the clusters using PCA and 2D scatter plot\npca = PCA(n_components=2)\npca_data = pca.fit_transform(standardized_data)\n\nplt.scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_labels)\nplt.title(\"Wine Data Clustering\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.show()"
  }
]